# docker-compose exec spark-master /spark/bin/spark-shell --master spark://spark-master:7077

# docker exec spark-master /spark/bin/spark-submit --packages org.mongodb.spark:mongo-spark-connector:10.0.5 --master spark://spark-master:7077 /opt/spark-data/spark_streaming.py

#SPARK_HOME = C:\apps\opt\spark-3.3.0-bin-hadoop3
#HADOOP_HOME = C:\apps\opt\spark-3.3.0-bin-hadoop3
#PYSPARK_PYTHON = python

version: '3.3.0'
services:
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    hostname: spark-master
    healthcheck:
      interval: 5s
      retries: 100
    ports:
      - "8080:8080"
      - "4040:4040"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=false
    volumes:
      - ./spark_streaming/:/opt/spark-data
#    networks:
#        - big-data-net

  spark-worker-1:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
    volumes:
      - ./spark_streaming/:/opt/spark-data
  spark-history-server:
      image: bde2020/spark-history-server:3.3.0-hadoop3.3
      container_name: spark-history-server
      depends_on:
        - spark-master
      ports:
        - "18081:18081"
      volumes:
        - /tmp/spark-events-local:/tmp/spark-events
  twitter-socket-host:
    image: twitter-socket-stream
    container_name: twitter-socket-host
    hostname: socket-host
    ports:
      - "5555:5555"
#networks:
#  big-data-net:
#   # Bridge Driver: Default network driver
#    #external: true
#    driver: bridge
#    ipam:
#      config:
#        - subnet: 172.22.0.0/24
#          gateway: 172.22.0.1